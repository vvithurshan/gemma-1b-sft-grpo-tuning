# gemma-1b-sft-grpo-tuning

# Gemma-3-1B Fine-Tuning with SFT and GRPO

This repository provides an end-to-end pipeline for post-training the Gemma-3-1B model using:

- Supervised Fine-Tuning (SFT)
- GRPO (Group Relative Policy Optimization)

The project demonstrates practical LLM alignment techniques including instruction tuning and reinforcement learning-based policy optimization.

---

## ðŸš€ Overview

Large Language Models require post-training to improve:

- Instruction-following
- Reasoning quality
- Response alignment
- Preference optimization

This repository implements:

1. Supervised Fine-Tuning (SFT) on instruction-style datasets
2. GRPO-based reinforcement learning for preference optimization
3. Efficient training using parameter-efficient techniques
4. Experiment tracking and reproducible configurations

---

