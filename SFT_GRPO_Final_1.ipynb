{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d4a095",
   "metadata": {},
   "source": [
    "# Google Tunix Hack - Train a Model to Show its Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a19b4",
   "metadata": {},
   "source": [
    "This notebook demonstrates the post-training of Gemma-3-1B-IT across diverse domains, including math, coding, science, and creative writing. Using Tunix‚ÄîGoogle‚Äôs new JAX-native library‚Äîwe implement a two-phase pipeline: Supervised Fine-Tuning (SFT) to establish a structured reasoning format, followed by Reinforcement Learning (RL) to enhance logical depth. Our approach emphasizes reproducibility and provides a clear end-to-end framework for generating verifiable reasoning traces before reaching a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea61c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pprint import pprint\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "from huggingface_hub import snapshot_download\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import model as gemma_lib\n",
    "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "from tunix.models.gemma3 import params as gemma_params\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from tunix.sft import peft_trainer\n",
    "from tunix.sft import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b070eb7",
   "metadata": {},
   "source": [
    "## Logging Into Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65872a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# print(\"Using env vars to login\")\n",
    "\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# print(\"nest_asyncio applied\")\n",
    "\n",
    "# # Only using wandb on TPU VM because it has strange bugs on Colab\n",
    "# !pip install -q wandb\n",
    "# import wandb\n",
    "# # Check if WANDB_API_KEY is set before logging in\n",
    "# if \"WANDB_API_KEY\" in os.environ and os.environ[\"WANDB_API_KEY\"]:\n",
    "#     wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "# else:\n",
    "#     print(\"WANDB_API_KEY not found. Skipping wandb login.\")\n",
    "\n",
    "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
    "#   kagglehub.login()\n",
    "\n",
    "# if \"HF_TOKEN\" in os.environ and os.environ[\"HF_TOKEN\"]:\n",
    "#     hf_token = os.environ[\"HF_TOKEN\"]\n",
    "#     !hf auth login --token \"$hf_token\"\n",
    "# else:\n",
    "#     print(\"HF_TOKEN not found. Skipping Hugging Face login.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9cda8a",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a407ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824010c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487b91a",
   "metadata": {},
   "source": [
    "The show_hbm_usage() function monitors High Bandwidth Memory (HBM) consumption across all local JAX devices (e.g., TPUs or GPUs). It retrieves memory statistics directly from the device runtime, formats the byte counts into human-readable units (MiB/GiB), and calculates the utilization percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ae9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  for d in jax.local_devices():\n",
    "    stats = d.memory_stats()\n",
    "    used = stats[\"bytes_in_use\"]\n",
    "    limit = stats[\"bytes_limit\"]\n",
    "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")\n",
    "\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f3278",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ece7f",
   "metadata": {},
   "source": [
    "This cell defines required variable and hyperparameters to conduct SFT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a427926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the post-training we are going to use gemma-3-1b-it Model\n",
    "# So, let's define the model id and its corresponding tokenizer\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
    "\n",
    "# Since we require maximum target length of 1200 (according to our dataset)\n",
    "# During training we are going to use a batch size of 8\n",
    "BATCH_SIZE = 8 \n",
    "MAX_TARGET_LENGTH = 1200 # Adjusted based on your TPU memory and model size.\n",
    "\n",
    "\n",
    "# Sharding\n",
    "# Model Setup\n",
    "NUM_DEVICES = len(jax.devices())\n",
    "\n",
    "# Adjust dimensions based on available GPUs\n",
    "# We define the shape as (FSDP, TP)\n",
    "if NUM_DEVICES == 8:\n",
    "  MESH_COUNTS = (1, 8) \n",
    "elif NUM_DEVICES == 4:\n",
    "  MESH_COUNTS = (1, 4)\n",
    "elif NUM_DEVICES == 2:\n",
    "  MESH_COUNTS = (1, 2)\n",
    "elif NUM_DEVICES == 1:\n",
    "  MESH_COUNTS = (1, 1)\n",
    "else:\n",
    "  raise ValueError(f\"Unsuppored Number of TPUs: {NUM_DEVICES}\")\n",
    "\n",
    "MESH = [\n",
    "    MESH_COUNTS,\n",
    "    (\"fsdp\", \"tp\"),\n",
    "]\n",
    "\n",
    "# LoRA/QLoRA Configuration\n",
    "RANK =64\n",
    "ALPHA = 64\n",
    "\n",
    "# ############## Train ############################\n",
    "# As the training time is one of the limiting factor, we are not going to \n",
    "# evaluate the model, which will increase the training time\n",
    "# so, the training fraction is set to 1.0. However, if evaluation required \n",
    "# the traning fraction can be adjected accordingly\n",
    "TRAIN_FRACTION = 0.99\n",
    "SFT_MAX_STEPS = 1000 # Maximum training steps\n",
    "EVAL_EVERY_N_STEPS = 2000 # evaluates the model every 200 steps\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# #################### Checkpoint saving #################\n",
    "# Following path are defined to store model checkpoints during training.\n",
    "SFT_FULL_CKPT_DIR = f\"{os.getcwd()}/tmp/sft/full_ckpts/\"\n",
    "SFT_CKPT_DIR = f\"{os.getcwd()}/tmp/sft/lora_ckpts/\"\n",
    "SFT_PROFILING_DIR = f\"{os.getcwd()}/tmp/sft/profiling/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7da4c",
   "metadata": {},
   "source": [
    "The following function is responsible for creating required folders for checkpoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import shutil\n",
    "\n",
    "def create_dir(path):\n",
    "  try:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    logging.info(f\"Created dir: {path}\")\n",
    "  except OSError as e:\n",
    "    logging.error(f\"Error creating directory '{path}': {e}\")\n",
    "\n",
    "\n",
    "create_dir(SFT_FULL_CKPT_DIR)\n",
    "create_dir(SFT_CKPT_DIR)\n",
    "create_dir(SFT_PROFILING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d883e",
   "metadata": {},
   "source": [
    "## Load model from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbeff9",
   "metadata": {},
   "source": [
    "In the following cell, we are downloading our model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_patterns = [\n",
    "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
    "]\n",
    "print(f\"Downloading {model_id} from Hugging Face...\")\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=model_id, ignore_patterns=ignore_patterns\n",
    ")\n",
    "print(f\"Model successfully downloaded to: {local_model_path}\")\n",
    "\n",
    "EOS_TOKENS = []\n",
    "generation_config_path = os.path.join(local_model_path, \"generation_config.json\")\n",
    "if os.path.exists(generation_config_path):\n",
    "  with open(generation_config_path, \"r\") as f:\n",
    "    generation_configs = json.load(f)\n",
    "  EOS_TOKENS = generation_configs.get(\"eos_token_id\", [])\n",
    "  print(f\"Using EOS token IDs: {EOS_TOKENS}\")\n",
    "\n",
    "print(\"\\n--- HBM Usage BEFORE Model Load ---\")\n",
    "# show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b821f",
   "metadata": {},
   "source": [
    "We first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, whcih can then be successfully loaded by the final sharded NNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CP_PATH = local_model_path\n",
    "\n",
    "if \"gemma-3-1b\" in model_id:\n",
    "    model_config = gemma_lib.ModelConfig.gemma3_1b_it()\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n",
    "with mesh:\n",
    "    base_model = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "        MODEL_CP_PATH, (model_config), mesh\n",
    "    )\n",
    "    # nnx.display(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd17aa9",
   "metadata": {},
   "source": [
    "## Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ede392",
   "metadata": {},
   "source": [
    "This snippet initializes the tokenizer for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
    "if tokenizer.eos_id() not in EOS_TOKENS:\n",
    "    EOS_TOKENS.append(tokenizer.eos_id())\n",
    "    print(f\"Using EOS token IDs: {EOS_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ad6fb",
   "metadata": {},
   "source": [
    "## Apply LoRA to the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ca192",
   "metadata": {},
   "source": [
    "The `get_lora_model` takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it is ready for distributed training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grpo notebook\n",
    "def get_lora_model(base_model, mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=(\n",
    "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "          \".*attn_vec_einsum\"\n",
    "      ),\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "  )\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA model \n",
    "sft_lora_model = get_lora_model(base_model, mesh=mesh,)\n",
    "# nnx.display(lora_model) # uncomment this if you want to dispaly the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6919383",
   "metadata": {},
   "source": [
    "## Load Datasets for SFT Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a2708",
   "metadata": {},
   "source": [
    "The following code is adapted from the Tunix library. Because Tunix does not natively provide SFT dataset preparation‚Äîspecifically for tokenization, padding, truncation, train-test splitting, and shuffling‚Äîthis notebook introduces a modified function to handle these preprocessing steps for our custom dataset.\n",
    "\n",
    "For our SFT training, we are going to use a dataset called VITHURSHAN/Selected_SFT_plus_Cascade-SFT-Stage-1 from Hugging Face\n",
    "https://huggingface.co/datasets/VITHURSHAN/Selected_SFT_plus_Cascade-SFT-Stage-1\n",
    "\n",
    "Since the dataset for both SFT contains multiple domains, system prompt for each domain must be generated. To streamline the training process, the datasets mentioned above contains the system prompt for each data point in a separate column. Therefore, system prompt for each domain is not required for this training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Data loading and preprocessing.\"\"\"\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from typing import Any\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from grain import python as grain\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.sft.peft_trainer import TrainingInput  \n",
    "\n",
    "\n",
    "INPUT_TEMPLATE_IT = {\n",
    "    \"prefix\": \"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\",\n",
    "    \"suffix\": \"\\n<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "}\n",
    "\n",
    "def create_datasets(\n",
    "    dataset_name: str,\n",
    "    global_batch_size: int,\n",
    "    max_target_length: int,\n",
    "    num_train_epochs: int | None,\n",
    "    tokenizer: tokenizer_lib.Tokenizer,\n",
    "    instruct_tuned: bool = False,\n",
    "    TRAIN_FRACTION: float = 1.0,\n",
    ") -> tuple[Iterable[TrainingInput], Iterable[TrainingInput]]:\n",
    "  \"\"\"Creates train and eval data iterator.\n",
    "\n",
    "  Args:\n",
    "    dataset_name: The name of the dataset to use.\n",
    "    global_batch_size: The global batch size to use for both train and eval.\n",
    "    max_target_length: The maximum length of the target sequence.\n",
    "    num_train_epochs: The number of epochs to use for training. If None, the\n",
    "      dataset will be repeated indefinitely.\n",
    "    tokenizer: The tokenizer to use for tokenizing the dataset.\n",
    "    instruct_tuned: Whether the dataset should be instruct tuned.\n",
    "    input_template: The input template to use for the dataset.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of train and eval data iterators.\n",
    "  \"\"\"\n",
    "\n",
    "  input_template = INPUT_TEMPLATE_IT\n",
    "\n",
    "  try:\n",
    "      raw_data = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "      if TRAIN_FRACTION == 1.0:\n",
    "          train_loader = _build_data_loader(\n",
    "          data_source=raw_data,\n",
    "          batch_size=global_batch_size,\n",
    "          num_epochs=num_train_epochs,\n",
    "          max_seq_len=max_target_length,\n",
    "          tokenizer=tokenizer,\n",
    "          input_template=input_template,\n",
    "          )\n",
    "          return train_loader, None\n",
    "      else:\n",
    "        split_data = raw_data.train_test_split(train_size=TRAIN_FRACTION, seed=42, shuffle=True)\n",
    "        train_ds, eval_ds = split_data[\"train\"], split_data[\"test\"]\n",
    "\n",
    "  except ValueError as e:\n",
    "      raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "\n",
    "  train_loader = _build_data_loader(\n",
    "      data_source=train_ds,\n",
    "      batch_size=global_batch_size,\n",
    "      num_epochs=num_train_epochs,\n",
    "      max_seq_len=max_target_length,\n",
    "      tokenizer=tokenizer,\n",
    "      input_template=input_template,\n",
    "  )\n",
    "  eval_loader = _build_data_loader(\n",
    "      data_source=eval_ds,\n",
    "      batch_size=global_batch_size,\n",
    "      num_epochs=1,\n",
    "      max_seq_len=max_target_length,\n",
    "      tokenizer=tokenizer,\n",
    "      input_template=input_template,\n",
    "  )\n",
    "  return train_loader, eval_loader\n",
    "\n",
    "\n",
    "def _build_data_loader(\n",
    "    *,\n",
    "    data_source: grain.RandomAccessDataSource,\n",
    "    batch_size: int,\n",
    "    num_epochs: int | None,\n",
    "    max_seq_len: int,\n",
    "    tokenizer: tokenizer_lib.Tokenizer,\n",
    "    input_template: dict[str, str],\n",
    ") -> grain.DataLoader:\n",
    "  \"\"\"Builds a data loader for the given data source.\"\"\"\n",
    "  return grain.DataLoader(\n",
    "      data_source=data_source,\n",
    "      sampler=grain.IndexSampler(\n",
    "          num_records=len(data_source),\n",
    "          num_epochs=num_epochs,\n",
    "          shard_options=grain.NoSharding(),\n",
    "          shuffle=True,\n",
    "          seed=42,\n",
    "      ),\n",
    "      worker_count=5,\n",
    "      operations=[\n",
    "          _Tokenize(tokenizer, input_template),\n",
    "          _BuildTrainInput(max_seq_len, tokenizer.pad_id()),\n",
    "          _FilterOverlength(max_seq_len),\n",
    "          grain.Batch(batch_size=batch_size, drop_remainder=True),\n",
    "      ],\n",
    "  )\n",
    "\n",
    "class _Tokenize(grain.MapTransform):\n",
    "    \"\"\"Tokenize the input.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: tokenizer_lib.Tokenizer, input_template: dict[str, str]\n",
    "    ):\n",
    "      self._tokenizer = tokenizer\n",
    "      self._input_template = input_template\n",
    "\n",
    "    def map(self, element: dict[str, Any]) -> tuple[np.ndarray, np.ndarray]:\n",
    "      \"\"\"Tokenize the input.\"\"\"\n",
    "      if \"question\" in element.keys():  \n",
    "        src_tokens = self._tokenizer.tokenize(\n",
    "            element[\"question\"], # .decode() removed\n",
    "            prefix=self._input_template[\"prefix\"].format(SYSTEM_PROMPT=element['system_prompt']),\n",
    "            suffix=self._input_template[\"suffix\"],\n",
    "            add_eos=False,\n",
    "        )\n",
    "        dst_tokens = self._tokenizer.tokenize(\n",
    "            element[\"response\"], add_eos=True # decode() removed\n",
    "        )\n",
    "      return src_tokens, dst_tokens\n",
    "\n",
    "\n",
    "class _BuildTrainInput(grain.MapTransform):\n",
    "    \"\"\"Build a TrainingInput from a tuple of source and destination tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len: int, pad_value: int | bool):\n",
    "      self._max_seq_len = max_seq_len\n",
    "      self._pad_value = pad_value\n",
    "\n",
    "    def map(self, tokens: tuple[np.ndarray, np.ndarray]) -> TrainingInput:\n",
    "      src_tokens, dst_tokens = tokens\n",
    "\n",
    "      # The input sequence fed to the model is simply the concatenation of the\n",
    "      # source and the destination.\n",
    "      tokens = np.concat([src_tokens, dst_tokens], axis=0)\n",
    "\n",
    "      # To prevent the model from updating based on the source (input)\n",
    "      # tokens, add a target mask to each input.\n",
    "      q_mask = np.zeros_like(src_tokens, dtype=np.bool)\n",
    "      a_mask = np.ones_like(dst_tokens, dtype=np.bool)\n",
    "      mask = np.concat([q_mask, a_mask], axis=0)\n",
    "\n",
    "      # If the input tokens sequence is smaller than the target sequence size,\n",
    "      # then pad it with pad tokens.\n",
    "      tokens = self._pad_up_to_max_len(tokens, self._pad_value)\n",
    "\n",
    "      # Don't want to perform the backward pass on the pad tokens.\n",
    "      mask = self._pad_up_to_max_len(mask, 0)\n",
    "\n",
    "      return TrainingInput(input_tokens=tokens, input_mask=mask)\n",
    "\n",
    "    def _pad_up_to_max_len(\n",
    "        self, input_tensor: np.ndarray, pad_value: int\n",
    "    ) -> np.ndarray:\n",
    "      \"\"\"Pad the given tensor up to sequence length of a batch.\"\"\"\n",
    "      seq_len = input_tensor.shape[0]\n",
    "      to_pad = np.maximum(self._max_seq_len - seq_len, 0)\n",
    "      return np.pad(\n",
    "          input_tensor,\n",
    "          [[0, to_pad]],\n",
    "          mode=\"constant\",\n",
    "          constant_values=pad_value,\n",
    "      )\n",
    "\n",
    "class _FilterOverlength(grain.FilterTransform):\n",
    "    \"\"\"Filter out overlength examples.\"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len: int):\n",
    "      self._max_seq_len = max_seq_len\n",
    "\n",
    "    def filter(self, element: TrainingInput) -> bool:\n",
    "      return element.input_tokens.shape[0] <= self._max_seq_len\n",
    "\n",
    "\n",
    "# Now call the above function to create train and eval dataset\n",
    "# Rebember, if you set TRAIN_FRACTION = 1.0, you will get None for sft_eval_ds\n",
    "# However, it does not affect the training process. \n",
    "\n",
    "# VITHURSHAN/Full_SFT_plus_Cascade-SFT-Stage-1\n",
    "sft_train_ds, sft_eval_ds = create_datasets(\"VITHURSHAN/SFT_Jan22\",\n",
    "                global_batch_size=BATCH_SIZE,\n",
    "                max_target_length=MAX_TARGET_LENGTH,\n",
    "                num_train_epochs=NUM_EPOCHS,\n",
    "                tokenizer=tokenizer,\n",
    "                instruct_tuned=True,\n",
    "                TRAIN_FRACTION=TRAIN_FRACTION,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf63380",
   "metadata": {},
   "source": [
    "`gen_model_input_fn` transforms raw training inputs into the structured format required by the transformer. It generates essential metadata, including a positional index and a causal attention mask, based on the input tokens' padding mask to ensure the model only attends to valid, preceding information during training.\n",
    "\n",
    "Key Components:\n",
    "pad_mask: Identifies non-padding tokens.\n",
    "\n",
    "positions: Maps the sequential order of tokens, ignoring padding.\n",
    "\n",
    "attention_mask: A causal mask that prevents the model from \"looking ahead\" at future tokens or attending to padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
    "  pad_mask = x.input_tokens != tokenizer.pad_id()\n",
    "  positions = utils.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = utils.make_causal_attn_mask(pad_mask)\n",
    "  return {\n",
    "      'input_tokens': x.input_tokens,\n",
    "      'input_mask': x.input_mask,\n",
    "      'positions': positions,\n",
    "      'attention_mask': attention_mask,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d38ba0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbc6b9",
   "metadata": {},
   "source": [
    "This code configures and executes the **Supervised Fine-Tuning (SFT)** pipeline for the model using **LoRA** (Low-Rank Adaptation).\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "* **Logging & Checkpointing:** Integrates **Weights & Biases (WandB)** and TensorBoard for real-time experiment tracking, with automated model saving every 250 steps.\n",
    "* **Optimization Suite:** Implements an **Optax** chain featuring a **warmup-cosine decay schedule** and **global gradient clipping** (set to 1.0) to stabilize training and prevent divergent gradients.\n",
    "* **Trainer Initialization:** Configures the `PeftTrainer` with gradient accumulation and a custom input function to handle the JAX-native data flow.\n",
    "* **Execution:** Runs the training loop within a distributed **JAX mesh** context, ensuring the session closes gracefully via `wandb.finish()` even if a crash occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "# ignore wandb errors\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "# lora_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "#     log_dir=f\"./tmp/sft/tensorboard/lora\", flush_every_n_steps=20\n",
    "# )\n",
    "\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=250,\n",
    "    max_to_keep=4,\n",
    ")\n",
    "# 1. Initialize WandB\n",
    "# wandb.init(\n",
    "#     project=\"gemma-tunix\",  # Change this to your project name\n",
    "#     name=f\"run-LORA\",\n",
    "#     config={\n",
    "#         \"eval_every_n_steps\": EVAL_EVERY_N_STEPS,\n",
    "#         \"max_steps\": SFT_MAX_STEPS,\n",
    "#         \"learning_rate\": 2e-4,\n",
    "#         \"method\": \"LORA\",\n",
    "#     }\n",
    "# )\n",
    "# wandb.init()\n",
    "\n",
    "sft_training_config = peft_trainer.TrainingConfig(\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=SFT_MAX_STEPS,\n",
    "    # metrics_logging_options=lora_logging_options, \n",
    "    checkpoint_root_directory=SFT_CKPT_DIR,\n",
    "    checkpointing_options=checkpointing_options,\n",
    ")\n",
    "\n",
    "# 1. Recommended LR: 2e-4 is the \"Gold Standard\" for LoRA\n",
    "# 1e-3 is often too high; 5e-5 is too slow.\n",
    "PEAK_LR = 2e-4\n",
    "\n",
    "# A. Define the Scheduler\n",
    "sft_lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=PEAK_LR, \n",
    "    # Warmup: Usually 3-5% of total steps. \n",
    "    warmup_steps=0.15 * SFT_MAX_STEPS,\n",
    "    decay_steps=SFT_MAX_STEPS,\n",
    "    end_value=PEAK_LR * 0.2  # Decay to 10% of peak (2e-5)\n",
    ")\n",
    "\n",
    "# B. Define the Optimizer WITH Gradient Clipping\n",
    "# Clipping prevents \"exploding gradients\" which ruin training runs instantly.\n",
    "sft_optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # <--- Crucial \"other thing\" to add\n",
    "    optax.adamw(learning_rate=sft_lr_schedule, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "sft_trainer = peft_trainer.PeftTrainer(\n",
    "    sft_lora_model, sft_optimizer, sft_training_config\n",
    ").with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "with mesh:\n",
    "    sft_trainer.train(sft_train_ds, sft_eval_ds)\n",
    "# try:\n",
    "#     with mesh:\n",
    "#         sft_trainer.train(sft_train_ds, sft_eval_ds)\n",
    "# finally:\n",
    "#     # Ensure run closes even if training crashes\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd023c8e",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd325e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tunix.generate import sampler as sampler_lib\n",
    "\n",
    "sft_sampler = sampler_lib.Sampler(\n",
    "    transformer=sft_lora_model,\n",
    "    tokenizer=tokenizer if \"gemma\" in model_id else tokenizer.tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=1700,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "# wandb.init()\n",
    "\n",
    "question =\"\"\"you are a good scientist. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "Which of the following will most likely to be true?\n",
    "\n",
    "Options:\n",
    "A: focused light beams like lasers can endanger pilots\n",
    "B: napkins can endanger pilots\n",
    "C: sweaters can endanger pilots\n",
    "D: teddy bears can endanger pilots\\n\"\"\"\n",
    "# question = \"write a python function to find the maximum from a given array, do not use any in-build methods. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags\"\n",
    "\n",
    "# question = \"\"\"you are a good summarizer. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final summary between <answer> and </answer> tags. \n",
    "# The transition to solid-state batteries (SSBs) represents a paradigm shift in automotive engineering, promising energy densities nearly double that of current liquid-electrolyte lithium-ion cells. Proponents argue that SSBs eliminate the risk of thermal runaway by replacing flammable liquid electrolytes with non-combustible ceramic or polymer separators. However, significant manufacturing hurdles remain. High-volume production is currently cost-prohibitive due to the sensitivity of solid electrolytes to moisture and the difficulty of maintaining consistent 'solid-to-solid' interface contact during the battery's expansion and contraction cycles. While companies like Toyota and QuantumScape claim commercial viability is imminent, skeptics maintain that the supply chain for specialized raw materials is at least a decade away from maturity.\n",
    "# \"\"\"\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "# question = \"\"\"you are a good math solver. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "# Vithurshan is installing solar panels on a laboratory roof. Each solar panel produces 250 watts of power. He installs 12 rows of panels, with 8 panels in each row. However, due to building shade, 4 panels in total only operate at 50% capacity, and 2 panels are completely broken.\n",
    "\n",
    "# How many total watts of power does the solar array produce?\n",
    "# \"\"\"\n",
    "\n",
    "# question = \"\"\"You are a good math solver. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "# I have two mangoes and three apples. How many fruits do I have?\"\"\"\n",
    "\n",
    "# question = \"\"\"you are a good story writer. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "# Write a story about a man who found a treasure in the jungle. The story should be 100 words long.\"\"\"\n",
    "\n",
    "# question = \"\"\"you are a good summarizer. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final summary between <answer> and </answer> tags. \n",
    "# Summarize the following text: after the WW2, USA and USSR were the two superpowers in the world. They were in a race to see who could build the most powerful weapon. The USA won the race by building the atomic bomb. The USSR lost the race by not having the technology to build the atomic bomb.\"\"\"\n",
    "\n",
    "\n",
    "input_batch = [\n",
    "TEMPLATE.format(question=question)\n",
    "]\n",
    "\n",
    "# input_batch = [\n",
    "#     question\n",
    "# ]\n",
    "out_data = sft_sampler(\n",
    "    input_strings=input_batch,\n",
    "    max_generation_steps=800,  # The number of steps performed when generating a response.\n",
    "    eos_tokens=EOS_TOKENS,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "for input_string, out_string in zip(input_batch, out_data.text):\n",
    "  print(f\"----------------------\")\n",
    "  print(f\"Prompt:\\n{input_string}\")\n",
    "  print(f\"Output:\\n{out_string}\")\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a857c21",
   "metadata": {},
   "source": [
    "## Clear Caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import jax\n",
    "\n",
    "# try:\n",
    "#     # Delete references\n",
    "#     del sft_lora_model, sft_optimizer, sft_lr_schedule, sft_trainer\n",
    "#     del sft_training_config, sft_train_ds, sft_eval_ds\n",
    "# except NameError:\n",
    "#     pass\n",
    "\n",
    "# # 1. Clear JIT/compilation caches\n",
    "# jax.clear_caches() \n",
    "\n",
    "# # 2. Force Python garbage collection\n",
    "# gc.collect() \n",
    "\n",
    "# # 3. Optional: Clear the live buffer cache (important for JAX)\n",
    "# # This is often needed to truly see memory return in nvidia-smi\n",
    "# for buf in jax.live_arrays():\n",
    "#     buf.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7497bb4",
   "metadata": {},
   "source": [
    "With the SFT phase complete, we now transition to Reinforcement Learning. We will utilize Group Relative Policy Optimization (GRPO), an algorithm specifically designed to enhance the reasoning capabilities of LLMs. As a variant of Proximal Policy Optimization (PPO), GRPO significantly reduces memory overhead by eliminating the need for a separate value function (critic) model. Instead, it generates a group of outputs for each prompt, evaluates them via a reward function, and updates the policy based on the relative advantage within that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3d1b2",
   "metadata": {},
   "source": [
    "# GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccc4be",
   "metadata": {},
   "source": [
    "This Part of the Notebook mainly utilizes the `grpo_gemma` example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9b764",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917277be",
   "metadata": {},
   "source": [
    "Let's define the configuration we are going to use for grpo training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be49727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== GRPO ======\n",
    "# === Generation during GRPO training ===\n",
    "MAX_PROMPT_LENGTH = 1100\n",
    "TOTAL_GENERATION_STEPS = 950\n",
    "# Important to keep a high-ish temperature for varied, diverse responses during\n",
    "# training.\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "# The number of times the policy generates multiple responses for a given prompt\n",
    "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
    "# paper. The \"group\" in GRPO comes from here.\n",
    "NUM_GENERATIONS = 4\n",
    "\n",
    "# === other GRPO configs ===\n",
    "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
    "NUM_ITERATIONS = 1\n",
    "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
    "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
    "# can increase unchecked.\n",
    "BETA = 0.08\n",
    "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
    "# stable updates.\n",
    "EPSILON = 0.2\n",
    "\n",
    "# ====== Training ======\n",
    "TRAIN_MICRO_BATCH_SIZE = 1\n",
    "TRAIN_FRACTION = 1.0\n",
    "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
    "NUM_BATCHES = 8000\n",
    "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
    "# increased to a max. of 330 (if batch size is 4).\n",
    "NUM_TEST_BATCHES = 64\n",
    "\n",
    "EVAL_EVERY_N_STEPS = 64  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
    "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
    "\n",
    "# Number of training steps.\n",
    "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "\n",
    "# === AdamW, warmup, cosine scheduler ===\n",
    "LEARNING_RATE = 3e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "# == Cosine decay with warmup scheduler ==\n",
    "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
    "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
    "# scheduler.\n",
    "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
    "# == Grad clipping ==\n",
    "# Grad clipping to prevent large gradients. Found this\n",
    "# important to keep KL divergence in check.\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# Checkpoint saving\n",
    "INTERMEDIATE_CKPT_DIR = f\"{os.getcwd()}/tmp/grpo/intermediate_ckpt/\"\n",
    "CKPT_DIR = f\"{os.getcwd()}/tmp/grpo/ckpts/\"\n",
    "SAVE_INTERVAL_STEPS = 25\n",
    "MAX_TO_KEEP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4abc7a",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0063a4d",
   "metadata": {},
   "source": [
    "The following function is inspired from the data processing function mention in the `grpo_gemma` notebook and slightly modified to handle the custom dataset. As mentioned above, the dataset itself contains the system prompt including formatting instructions with <reasoning>, </reasoning>, <answer>, </answer> tags. There is no need for a user to define this during the GRPO training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4124b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import grain\n",
    "\n",
    "# Define the that contains system_prompt and question\n",
    "# both system_prompt and question will be coming from the dataset\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "def get_dataset(data_dir, split=\"train\", TRAIN_MICRO_BATCH_SIZE = 1, NUM_BATCHES=3738):\n",
    "\n",
    "    rl_data = load_dataset(data_dir)\n",
    "    dataset = (\n",
    "        grain.MapDataset.source(rl_data[split])\n",
    "        .shuffle(seed=42)\n",
    "        .map(\n",
    "            lambda x: {\n",
    "                \"domain\": x[\"domain\"],\n",
    "                \"prompts\": TEMPLATE.format\n",
    "                (\n",
    "                    system_prompt=x['system_prompt'],\n",
    "                    question=x[\"question\"],\n",
    "                ),\n",
    "                \"question\": x[\"question\"],\n",
    "                \"answer\": x[\"answer\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n",
    "\n",
    "# Let's call the function `get_dataset` to create a dataset\n",
    "dataset = get_dataset(\"VITHURSHAN/RL_With_Cascade\",\n",
    "                      split=\"train\",\n",
    "                      TRAIN_MICRO_BATCH_SIZE=TRAIN_MICRO_BATCH_SIZE*2,\n",
    "                      NUM_BATCHES=NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec56adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the TRAIN_FRACTION == 1.0, there is no val_set\n",
    "# it is similar to what we created during SFT\n",
    "# considering the limited time, we are not going to have any val_dataset\n",
    "if TRAIN_FRACTION == 1.0:\n",
    "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
    "  val_dataset = None\n",
    "else:\n",
    "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
    "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
    "\n",
    "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
    "\n",
    "# Finally, print the number of batches in the dataset\n",
    "dataset_lengths = (\n",
    "    len(train_dataset),\n",
    "    len(val_dataset) if val_dataset is not None else 0,\n",
    ")\n",
    "print(f\"dataset contains {dataset_lengths} of batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fd348",
   "metadata": {},
   "source": [
    "## Load Policy and Reference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8670b9",
   "metadata": {},
   "source": [
    "The policy model is the primary model undergoing training and weight updates. In contrast, the reference model remains frozen and is used to calculate KL divergence. This constraint prevents the policy from deviating too drastically from its original distribution, ensuring training stability.\n",
    "\n",
    "In this configuration, we use the base model as our reference and the `sft_lora_model` as our policy. Because the policy utilizes LoRA (Low-Rank Adaptation), only the lightweight adapter weights are updated, significantly reducing the computational footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_CP_PATH = local_model_path\n",
    "# wandb.init()\n",
    "# if \"gemma-3-1b\" in model_id:\n",
    "#     model_config = gemma_lib.ModelConfig.gemma3_1b_it()\n",
    "# else:\n",
    "#     raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "# mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n",
    "# with mesh:\n",
    "#     base_model = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "#         MODEL_CP_PATH, (model_config), mesh\n",
    "#     )\n",
    "#     # nnx.display(base_model)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55950ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy model\n",
    "# wandb.finish()\n",
    "# wandb.init()\n",
    "lora_policy = get_lora_model(base_model, mesh=mesh)\n",
    "# sft_lora = get_lora_model(base_model, mesh=mesh)\n",
    "# nnx.display(lora_policy)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint from previous run\n",
    "from flax import nnx\n",
    "\n",
    "# wandb.init()\n",
    "ckp_path = f\"{SFT_CKPT_DIR}/{SFT_MAX_STEPS}/model_params\"\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(ckp_path, target=abs_params)\n",
    "\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(\n",
    "        lambda a, b: b,\n",
    "        nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        trained_lora_params\n",
    "    ),\n",
    ")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38cdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tunix.generate import sampler as sampler_lib\n",
    "\n",
    "ckp_sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer if \"gemma\" in model_id else tokenizer.tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=1700,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "# wandb.init()\n",
    "\n",
    "question =\"\"\"you are a good scientist. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "Which of the following will most likely to be true?\n",
    "\n",
    "Options:\n",
    "A: focused light beams like lasers can endanger pilots\n",
    "B: napkins can endanger pilots\n",
    "C: sweaters can endanger pilots\n",
    "D: teddy bears can endanger pilots\\n\"\"\"\n",
    "question = \"write a python function to find the maximum from a given array, do not use any in-build methods. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags\"\n",
    "\n",
    "# question = \"\"\"you are a good summarizer. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final summary between <answer> and </answer> tags. \n",
    "# The transition to solid-state batteries (SSBs) represents a paradigm shift in automotive engineering, promising energy densities nearly double that of current liquid-electrolyte lithium-ion cells. Proponents argue that SSBs eliminate the risk of thermal runaway by replacing flammable liquid electrolytes with non-combustible ceramic or polymer separators. However, significant manufacturing hurdles remain. High-volume production is currently cost-prohibitive due to the sensitivity of solid electrolytes to moisture and the difficulty of maintaining consistent 'solid-to-solid' interface contact during the battery's expansion and contraction cycles. While companies like Toyota and QuantumScape claim commercial viability is imminent, skeptics maintain that the supply chain for specialized raw materials is at least a decade away from maturity.\n",
    "# \"\"\"\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "question = \"\"\"you are a good math solver. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "Vithurshan is installing solar panels on a laboratory roof. Each solar panel produces 250 watts of power. He installs 12 rows of panels, with 8 panels in each row. However, due to building shade, 4 panels in total only operate at 50% capacity, and 2 panels are completely broken.\n",
    "\n",
    "How many total watts of power does the solar array produce?\n",
    "\"\"\"\n",
    "\n",
    "# question = \"\"\"You are a good math solver. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "# I have two mangoes and three apples. How many fruits do I have?\"\"\"\n",
    "\n",
    "# question = \"\"\"you are a good story writer. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final answer between <answer> and </answer> tags. \n",
    "# Write a story about a man who found a treasure in the jungle. The story should be 100 words long.\"\"\"\n",
    "\n",
    "# question = \"\"\"you are a good summarizer. Think carefully and write your reasoning between <reasoning> and </reasoning> tags and final summary between <answer> and </answer> tags. \n",
    "# Summarize the following text: after the WW2, USA and USSR were the two superpowers in the world. They were in a race to see who could build the most powerful weapon. The USA won the race by building the atomic bomb. The USSR lost the race by not having the technology to build the atomic bomb.\"\"\"\n",
    "\n",
    "\n",
    "input_batch = [\n",
    "TEMPLATE.format(question=question)\n",
    "]\n",
    "\n",
    "# input_batch = [\n",
    "#     question\n",
    "# ]\n",
    "out_data = ckp_sampler(\n",
    "    input_strings=input_batch,\n",
    "    max_generation_steps=800,  # The number of steps performed when generating a response.\n",
    "    eos_tokens=EOS_TOKENS,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "for input_string, out_string in zip(input_batch, out_data.text):\n",
    "  print(f\"----------------------\")\n",
    "  print(f\"Prompt:\\n{input_string}\")\n",
    "  print(f\"Output:\\n{out_string}\")\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4120a",
   "metadata": {},
   "source": [
    "## Define Reward Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbcf52a",
   "metadata": {},
   "source": [
    "To effectively improve the reasoning capabilities of the model during the reinforcement learning (RL) stage, this approach employs four complementary reward functions. Each reward targets a distinct aspect of model behavior, collectively guiding the model toward producing well-structured, logically sound, and high-quality responses. The combination of these rewards balances strict structural constraints with semantic quality and reference alignment.\n",
    "\n",
    "\n",
    "1. **Reward for Correct Formatting** - inspired from grpo_gemma notebook\n",
    "\n",
    "The first reward function enforces strict adherence to the required output format, which is essential for downstream evaluation and interpretability. The model is expected to generate outputs that strictly follow the predefined structure: <reasoning>reasoning</reasoning><answer>answer</answer>\n",
    "\n",
    "2. **Reward for Approximate Formatting** - inspired from grpo_gemma notebook\n",
    "\n",
    "While strict formatting is important, overly harsh penalties can hinder exploration during RL, particularly for smaller models. To mitigate this, a soft or approximate formatting reward is introduced.\n",
    "\n",
    "This reward provides partial credit when the model output is close to the desired format but contains minor deviations, such as:\n",
    "\n",
    "- Missing or malformed angle brackets\n",
    "\n",
    "- Incorrect capitalization of tags\n",
    "\n",
    "- Minor ordering issues (e.g., answer preceding reasoning)\n",
    "\n",
    "- Additional whitespace or newline inconsistencies\n",
    "\n",
    "3. **Reinforcement Learning with Reference Probability Reward (RLPR)** evaluates reasoning quality without external verifiers by using the model‚Äôs own confidence as the reward signal. For a given prompt, the model computes the token-level probability of a high-quality reference reasoning and answer. A higher probability indicates that the model internally judges the reasoning process as more likely to lead to the correct answer. During training, reinforcement learning maximizes this probability-based reward, encouraging reasoning trajectories that naturally align with correct outcomes across both structured and open-ended domains.\n",
    "\n",
    "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains (https://arxiv.org/pdf/2506.18254)\n",
    "\n",
    "4. **LLM-as-a-Judge Reward**\n",
    "\n",
    "The final reward function employs a large, external LLM as an automated evaluator to assess the quality of the model‚Äôs outputs. Given the input prompt and the model-generated response, the judge model evaluates multiple qualitative dimensions, including:\n",
    "\n",
    "- Correctness of the final answer\n",
    "\n",
    "- Logical coherence and completeness of the reasoning trace\n",
    "\n",
    "- Relevance to the input prompt\n",
    "\n",
    "- Clarity and usefulness of the explanation\n",
    "\n",
    "The judge model outputs a scalar score or categorical rating, which is then converted into a reward signal for RL optimization. This reward is particularly valuable for open-ended tasks such as creative writing, summarization, ideation, and story generation, where exact reference matching is infeasible.\n",
    "\n",
    "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains (https://arxiv.org/abs/2507.17746)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13cbc7",
   "metadata": {},
   "source": [
    "**NOTE**: While the cell below includes the code for the RLPR (Reinforcement Learning with Reference Probability) Reward, it will not be active during this run. Since RLPR requires real-time tokenization and forward passes to generate logits, omitting it allows us to optimize training speed and remain within the competition's 9-hour TPU time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from tunix.sft import utils\n",
    "import jax\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "# import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "# ========================Match format=================================\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "def match_format_exactly(response) -> float:\n",
    "    return -1 if match_format.search(response) is None else 1.0\n",
    "\n",
    "# ========================Partial match=================================\n",
    "def match_format_approximately(response) -> float:\n",
    "\n",
    "    score = 0\n",
    "    # Count how many keywords are seen - we penalize if too many!\n",
    "    # If we see 1, then plus some points!\n",
    "    score += 0.2 if response.count(reasoning_start) == 1 else -0.2\n",
    "    score += 0.2 if response.find(reasoning_start) == 0 else -0.2\n",
    "    score += 0.2 if response.count(reasoning_end) == 1 else -0.2\n",
    "    score += 0.2 if response.count(solution_start) == 1 else -0.2\n",
    "    score += 0.2 if response.count(solution_end) == 1 else -0.2\n",
    "    \n",
    "    return score\n",
    "\n",
    "# ======================== RLPR =================================\n",
    "def get_token_probabilities( \n",
    "    model: nnx.Module,\n",
    "    tokenizer,\n",
    "    sequence: str,\n",
    "    reference_answer: str,\n",
    ") -> List[float]:\n",
    "\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: model\n",
    "        tokenizer: tokenizer\n",
    "        sequence: str (Reasoning + Answer)\n",
    "        reference_answer: str (Answer only)\n",
    "    returns:\n",
    "        probs: List[float]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 1. Tokenize precisely to find the split point\n",
    "    # We slice the string to get reasoning to ensure we match the sequence prefix exactly\n",
    "    if sequence.endswith(reference_answer):\n",
    "        reasoning = sequence[:-len(reference_answer)]\n",
    "    else:\n",
    "        reasoning = sequence.replace(reference_answer, \"\")\n",
    "    tokenized_sequence = tokenizer.tokenize(sequence, add_eos=False)\n",
    "    # tokenized_answer = tokenizer.tokenize(reference_answer, add_eos=False)\n",
    "    # The index in tokenized_sequence where the answer begins\n",
    "    # Both share the same BOS token, so the length of the reasoning IDs\n",
    "    # is the correct starting index for the answer.\n",
    "\n",
    "    if not reasoning:\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        tokenized_reasoning = tokenizer.tokenize(reasoning, add_eos=False)\n",
    "        start_idx = tokenized_reasoning.shape[0]\n",
    "    # 2. Setup inputs\n",
    "    pad_mask = tokenized_sequence != tokenizer.pad_id()\n",
    "    pad_mask = jnp.expand_dims(pad_mask, axis=0)\n",
    "    positions = utils.build_positions_from_mask(pad_mask)\n",
    "    attention_mask = utils.make_causal_attn_mask(pad_mask)\n",
    "\n",
    "    # 3. Forward Pass\n",
    "    logits = model(\n",
    "        jnp.expand_dims(tokenized_sequence, axis=0),\n",
    "        positions,\n",
    "        None,\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "    # 4. Extract Log Probs\n",
    "    logits = jnp.squeeze(logits[0], axis=0) \n",
    "    # Logits at i predict token at i+1. Shift both to align.\n",
    "    log_probs = jax.nn.log_softmax(logits[:-1], axis=-1)\n",
    "    target_ids = tokenized_sequence[1:]\n",
    "\n",
    "    # Get log probs for the actual tokens that appeared\n",
    "    log_probs_all = jnp.take_along_axis(\n",
    "        log_probs, \n",
    "        target_ids[..., None], \n",
    "        axis=-1\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    # 5. Extract Answer Slice\n",
    "    # Since log_probs_all starts from the 2nd token (index 1 of sequence),\n",
    "    # the answer starting at start_idx in the sequence is at start_idx - 1 here.\n",
    "    ans_log_probs = log_probs_all[start_idx - 1:]\n",
    "\n",
    "    # Convert to probabilities\n",
    "    probs = jnp.exp(ans_log_probs)\n",
    "\n",
    "    return probs\n",
    "\n",
    "def calculate_probability_reward(\n",
    "    model: nnx.Module,\n",
    "    tokenizer,\n",
    "    reasoning: Optional[str],\n",
    "    reference_answer: str,\n",
    ") -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: model\n",
    "        tokenizer: tokenizer\n",
    "        reasoning: Optional[str] = None\n",
    "        reference_answer: Optional[str] = None\n",
    "    returns:\n",
    "        r: float\n",
    "    \"\"\"\n",
    "\n",
    "    if reasoning:\n",
    "        o_prime = f\"{reasoning} {reference_answer}\"\n",
    "    else:\n",
    "        o_prime = reference_answer\n",
    "\n",
    "    probability_of_y_star = get_token_probabilities(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        o_prime,\n",
    "        reference_answer,\n",
    "    )\n",
    "\n",
    "    if probability_of_y_star.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_probability = jnp.mean(probability_of_y_star)\n",
    "\n",
    "    return average_probability\n",
    "\n",
    "def calculate_debiased_reward(\n",
    "    model: nnx.Module,\n",
    "    tokenizer,\n",
    "    reasoning: str,\n",
    "    reference_answer: str,\n",
    ") -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: model\n",
    "        tokenizer: tokenizer\n",
    "        reasoning: str\n",
    "        reference_answer: str\n",
    "    returns:\n",
    "        r: float\n",
    "    \"\"\"\n",
    "    r = calculate_probability_reward(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        reasoning,\n",
    "        reference_answer,\n",
    "    )\n",
    "\n",
    "    r_prime = calculate_probability_reward(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        None,\n",
    "        reference_answer,\n",
    "    )\n",
    "\n",
    "    r_hat = r - r_prime\n",
    "    r_scaled = r_hat * 5\n",
    "    r_final = jnp.clip(r_scaled, a_min=0.0, a_max=1.0)\n",
    "\n",
    "    return float(r_final)\n",
    "\n",
    "# rlpr main\n",
    "def extract_reasoning_and_answer(response: str):\n",
    "\n",
    "    reasoning = re.search(rf'{reasoning_start}(.*?){reasoning_end}', response, re.DOTALL)\n",
    "    answer = re.search(rf'{solution_start}(.*?){solution_end}', response, re.DOTALL)\n",
    "\n",
    "    if reasoning and answer:\n",
    "      return reasoning.group(1), answer.group(1)\n",
    "    elif reasoning:\n",
    "      return reasoning.group(1), None\n",
    "    elif answer:\n",
    "      return None, answer.group(1)\n",
    "    else:\n",
    "      return None, None\n",
    "\n",
    "def rlpr(reasoning, reference_answer):\n",
    "    return calculate_debiased_reward(\n",
    "      lora_policy,\n",
    "      tokenizer,\n",
    "      reasoning,\n",
    "      reference_answer,\n",
    "    )\n",
    "\n",
    "# ========================LLM as Judge=================================\n",
    "# Prompts with Rubrics for reward function\n",
    "# Rubrics for Coding Math and Science\n",
    "LLM_AS_JUDGE_PROMPT_MATH_SCIENCE_CODING = \"\"\"\n",
    "You are a rigorous technical grader. Your task is to evaluate a model's response based on a Prompt and a Ground Truth answer.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Correctness (Critical):** Does the text inside <answer> matches the Ground Truth? \n",
    "2. **Logic (Essential):** Does the <reasoning> trace logically lead to the answer without hallucinations or \"looping\" logic?\n",
    "3. **Format (Required):** Are the <reasoning> and <answer> tags used correctly?\n",
    "\n",
    "### Scoring Rubric:\n",
    "- **2.0:** Perfect logic, correct final answer, and proper formatting.\n",
    "- **1.5:** Correct final answer and proper format, but the reasoning is slightly disorganized or contains minor fluff.\n",
    "- **1.0:** Correct final answer, but the reasoning is logically flawed, non-existent, or hallucinated.\n",
    "- **0.5:** Wrong final answer, but the reasoning shows significant effort and follows the correct methodology.\n",
    "- **0.0:** Wrong final answer and nonsensical/empty reasoning.\n",
    "\n",
    "### Inputs:\n",
    "- **Prompt:** {prompt}\n",
    "- **Model Response:** {response}\n",
    "- **Ground Truth:** {ground_truth}\n",
    "\n",
    "### Output Instruction:\n",
    "Examine the response carefully. Provide **ONLY** the numerical score as a float (e.g., 1.5 or 2.0). Do not include any text, explanations, or labels.\n",
    "\"\"\"\n",
    "\n",
    "# Rubrics for Story Generation\n",
    "LLM_AS_JUDGE_PROMPT_STORY = \"\"\"\n",
    "You are a professional literary critic. Evaluate the story based on the prompt.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Narrative Arc:** Does the <reasoning> show a clear plan (intro, conflict, climax) that the story follows?\n",
    "2. **Creativity:** Is the prose engaging, or is it repetitive and clich√©?\n",
    "3. **Consistency:** Do characters and settings stay consistent between the reasoning and the final story?\n",
    "\n",
    "### Scoring Rubric:\n",
    "- **2.0:** Excellent storytelling, vivid imagery, and a clear logical plan in the reasoning tags.\n",
    "- **1.5:** Good story, but the reasoning is thin or the prose uses too many repetitive \"filler\" words.\n",
    "- **1.0:** The story is coherent but boring/generic, or the reasoning doesn't match the output.\n",
    "- **0.5:** Fragmented story or major contradictions between the plan and the final text.\n",
    "- **0.0:** Non-sensical, offensive, or fails to follow the prompt entirely.\n",
    "\n",
    "### Inputs:\n",
    "- **Prompt:** {prompt}\n",
    "- **Response:** {response}\n",
    "\n",
    "### Output Instruction:\n",
    "Provide ONLY the numerical score as a float (0.0 - 2.0). No text.\n",
    "\"\"\"\n",
    "\n",
    "# Rubrics for Creative Ideation\n",
    "LLM_AS_JUDGE_PROMPT_IDEATION = \"\"\"\n",
    "You are an innovation consultant. Evaluate the ideas generated.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Originality:** Are these \"outside-the-box\" ideas, or just the most obvious solutions?\n",
    "2. **Feasibility:** Are the ideas actionable and relevant to the prompt?\n",
    "3. **Reasoning Quality:** Does the <reasoning> explore different angles before settling on the ideas in <answer>?\n",
    "\n",
    "### Scoring Rubric:\n",
    "- **2.0:** Diverse, high-quality, and unique ideas with thorough brainstorming in reasoning.\n",
    "- **1.5:** Good ideas, but they feel somewhat similar to each other.\n",
    "- **1.0:** Obvious or \"boring\" ideas that don't show much creative effort.\n",
    "- **0.0:** Repetitive ideas (listing the same thing twice) or irrelevant suggestions.\n",
    "\n",
    "### Inputs:\n",
    "- **Prompt:** {prompt}\n",
    "- **Response:** {response}\n",
    "\n",
    "### Output Instruction:\n",
    "Provide ONLY the numerical score as a float (0.0 - 2.0). No text.\n",
    "\"\"\"\n",
    "\n",
    "# Rubrics for summarization\n",
    "LLM_AS_JUDGE_PROMPT_SUMMARIZATION = \"\"\"\n",
    "You are an expert editor. Evaluate the summary of the provided text.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Factuality:** Does the summary contain ONLY information present in the source?\n",
    "2. **Density:** Is the summary concise while keeping all \"key points\"?\n",
    "3. **Reasoning:** Does the <reasoning> correctly identify the main entities and themes before summarizing?\n",
    "\n",
    "### Scoring Rubric:\n",
    "- **2.0:** Perfect summary: concise, factual, and covers all core points.\n",
    "- **1.5:** Factual summary, but misses one minor detail or is slightly too wordy.\n",
    "- **1.0:** Contains a minor factual hallucination or misses a major key point.\n",
    "- **0.0:** Major hallucinations, or the summary is longer than the original text.\n",
    "\n",
    "### Inputs:\n",
    "- **Source Text:** {prompt}\n",
    "- **Summary Response:** {response}\n",
    "\n",
    "### Output Instruction:\n",
    "Provide ONLY the numerical score as a float (0.0 - 2.0). No text.\n",
    "\"\"\" \n",
    "\n",
    "# Rubrics for Creative Writing\n",
    "LLM_AS_JUDGE_PROMPT_CREATIVE = \"\"\"\n",
    "You are an expert editor and literary critic. \n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **The Plan (<reasoning>):** Does the model outline a creative strategy (e.g., tone, structure, or plot points) before writing?\n",
    "2. **Execution (<answer>):** Is the writing engaging, vivid, and free of repetitive \"AI-style\" filler?\n",
    "3. **Prompt Adherence:** Did the model follow all constraints of the creative prompt?\n",
    "\n",
    "### Scoring Rubric:\n",
    "- **2.0:** Excellent. The reasoning shows a deep plan, and the writing is professional and creative.\n",
    "- **1.5:** Good. The writing is solid, but the reasoning is brief or the prose is slightly generic.\n",
    "- **1.0:** Functional. The text is coherent but lacks creativity or the reasoning is disconnected from the output.\n",
    "- **0.5:** Poor. Significant repetition, boring prose, or failed to use tags correctly.\n",
    "- **0.0:** Fail. Nonsensical, off-topic, or empty tags.\n",
    "\n",
    "### Inputs:\n",
    "- **Prompt:** {prompt}\n",
    "- **Response:** {response}\n",
    "\n",
    "### Output Instruction:\n",
    "Provide ONLY the numerical score as a float (0.0 - 2.0). No text.\n",
    "\"\"\"\n",
    "\n",
    "# Rubrics for General Reasoning Questions\n",
    "LLM_AS_JUDGE_PROMPT_GENERAL = \"\"\"\n",
    "You are an expert evaluator of general-purpose AI assistants.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Intent Analysis (<reasoning>):** Does the model correctly identify the user's core intent and break down the problem logically before answering?\n",
    "2. **Helpfulness & Clarity (<answer>):** Is the final response comprehensive, accurate, and structured in a way that is easy to read?\n",
    "3. **Instruction Following:** Did the model satisfy all explicit constraints (e.g., \"list format,\" \"concise,\" \"explain like I'm 5\")?\n",
    "\n",
    "### Scoring Rubric:\n",
    "- **2.0:** Excellent. The reasoning shows deep understanding of the user's goal. The answer is precise, high-quality, and follows all instructions perfectly.\n",
    "- **1.5:** Good. The answer is helpful and correct, but the reasoning is slightly generic or the writing style is verbose.\n",
    "- **1.0:** Functional. The response addresses the prompt, but misses minor constraints or the reasoning is disconnected from the final output.\n",
    "- **0.5:** Poor. Misses the core intent, contains hallucinations, or fails to use the required tags correctly.\n",
    "- **0.0:** Fail. Nonsensical, off-topic, harmful, or empty tags.\n",
    "\n",
    "### Inputs:\n",
    "- **Prompt:** {prompt}\n",
    "- **Response:** {response}\n",
    "\n",
    "### Output Instruction:\n",
    "Provide ONLY the numerical score as a float (0.0 - 2.0). No text.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the Judge Model \n",
    "# We are going to use Gemini-3 flash as our judge model\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyBH0jLu-8wiYwNUvSfqpJJqQcwr48x7ZP8\"\n",
    "\n",
    "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    try:\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        llm_judge = \"gemma-3-27b-it\"\n",
    "        # llm_judge = \"gemini-3-flash-preview\"\n",
    "        gemini_model = genai.GenerativeModel(llm_judge)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to initialize Gemini model: {e}\")\n",
    "        gemini_model = None\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY not found; skipping Gemini model initialization.\")\n",
    "    gemini_model = None\n",
    "\n",
    "# Since our dataset has a column that stores the domain information,\n",
    "# we can choose the correct prompt based on the domain\n",
    "\n",
    "def llm_as_judge(domain, prompt, response, ground_truth):\n",
    "    if gemini_model is None:\n",
    "        return 0.0\n",
    "\n",
    "    LLM_AS_JUDGE_PROMPT = None\n",
    "\n",
    "    # basic science, coding, gsm8k\n",
    "    if domain == \"basic_science\" or \\\n",
    "        domain == \"coding_data\" or \\\n",
    "            domain == \"gsm8k\":\n",
    "\n",
    "      LLM_AS_JUDGE_PROMPT = LLM_AS_JUDGE_PROMPT_MATH_SCIENCE_CODING.format(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "        ground_truth=ground_truth,\n",
    "      )\n",
    "    # creative ideation\n",
    "    elif domain == \"creative_ideation\":\n",
    "      LLM_AS_JUDGE_PROMPT = LLM_AS_JUDGE_PROMPT_IDEATION.format(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "      )\n",
    "\n",
    "    # story generation\n",
    "    elif domain == \"story_generation\":\n",
    "      LLM_AS_JUDGE_PROMPT = LLM_AS_JUDGE_PROMPT_STORY.format(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "      )\n",
    "\n",
    "    # creative writing\n",
    "    elif domain == \"creative_writing\":\n",
    "      LLM_AS_JUDGE_PROMPT = LLM_AS_JUDGE_PROMPT_CREATIVE.format(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "      )\n",
    "    # summarization\n",
    "    elif domain == \"summarization\":\n",
    "      LLM_AS_JUDGE_PROMPT = LLM_AS_JUDGE_PROMPT_SUMMARIZATION.format(\n",
    "        prompt=prompt,\n",
    "        response=response,\n",
    "      )\n",
    "\n",
    "    # general\n",
    "    elif domain == \"general\":\n",
    "        LLM_AS_JUDGE_PROMPT = LLM_AS_JUDGE_PROMPT_GENERAL.format(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "        )\n",
    "    else:\n",
    "      return 0.0\n",
    "\n",
    "    # call LLM\n",
    "    try:\n",
    "        response = gemini_model.generate_content(LLM_AS_JUDGE_PROMPT)\n",
    "        response_text = response.text\n",
    "        \n",
    "        # Finds the first integer or decimal in the response\n",
    "        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", response_text)\n",
    "        \n",
    "        if match:\n",
    "            reward = float(match.group())\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catching broader API errors (quota, safety filters, etc.)\n",
    "        reward = 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "# GRPO\n",
    "def GRPO_Rewards(prompts, completions, answer, **kwargs):\n",
    "\n",
    "    domain = kwargs[\"domain\"][0]\n",
    "    question = kwargs[\"question\"][0]\n",
    "    prompt = prompts[0]\n",
    "    responses = completions\n",
    "    scores = []\n",
    "    log_buffer = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # Initialize all components to 0.0\n",
    "        reasoning, extracted_answer = None, None\n",
    "        reward_for_rlpr = 0.0\n",
    "        reward_for_llm_as_judge = 0.0\n",
    "        reward_for_format_aptly = 0.0\n",
    "        reward_for_format_exactly = 0.0\n",
    "        reward_for_format_exactly = match_format_exactly(response)\n",
    "        if reward_for_format_exactly == 1.0:\n",
    "            # pass\n",
    "            reasoning, extracted_answer = extract_reasoning_and_answer(response)\n",
    "            # reward_for_rlpr = rlpr(reasoning, extracted_answer) # it is not used due to time limit\n",
    "            reward_for_llm_as_judge = llm_as_judge(domain, prompt, response, answer[0])\n",
    "\n",
    "        else:\n",
    "            reward_for_rlpr = 0.0\n",
    "            reward_for_llm_as_judge = 0.0\n",
    "            reward_for_format_aptly = match_format_approximately(response)\n",
    "\n",
    "        total_reward = reward_for_format_exactly + \\\n",
    "                      reward_for_format_aptly + \\\n",
    "                      reward_for_rlpr + \\\n",
    "                      reward_for_llm_as_judge\n",
    "\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        log_buffer.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"domain\": domain,\n",
    "                \"question\": question,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"answer\": answer[0],\n",
    "                \"rewards\": \n",
    "                {\n",
    "                    \"reward_for_format_exactly\": reward_for_format_exactly,\n",
    "                    \"reward_for_format_aptly\": reward_for_format_aptly,\n",
    "                    \"reward_for_llm_as_judge\": reward_for_llm_as_judge,\n",
    "                    \"reward_for_rlpr\": reward_for_rlpr,\n",
    "                    \"total_reward\": total_reward,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    # write in a json\n",
    "    with open(\"grpo_log.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for entry in log_buffer:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0e32c",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ckpt saving\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "# metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "#     log_dir=\"/tmp/grpo/tensorboard/grpo\", flush_every_n_steps=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, learning rate scheduler, gradient clipping\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "if MAX_GRAD_NORM is not None:\n",
    "  optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
    "      optimizer,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        # metrics logging\n",
    "        # metrics_logging_options=metrics_logging_options,\n",
    "        # checkpoint saving\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=EOS_TOKENS,\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699d40f",
   "metadata": {},
   "source": [
    "## Setting Up the GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=sft_lora_model, \n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "# GRPO Trainer\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        GRPO_Rewards\n",
    "    ],\n",
    "    algo_config=grpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7d592",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25250121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"tunix-grpo\"\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "# wandb.init()\n",
    "with mesh:\n",
    "  grpo_trainer.train(train_dataset, val_dataset)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fad65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
